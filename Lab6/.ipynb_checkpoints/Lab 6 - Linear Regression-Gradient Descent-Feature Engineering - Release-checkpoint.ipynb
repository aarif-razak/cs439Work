{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Student Information</h3> Please provide information about yourself.<br>\n",
    "<b>Name</b>: Aarif Razak<br>\n",
    "<b>NetID</b>:  ahr58 <br>\n",
    "<b>Recitation (01/02/90/91)</b>: 02<br>\n",
    "<b>Notes to Grader</b> (optional):<br>\n",
    "<br><br>\n",
    "<b>IMPORTANT</b>\n",
    "Your work will not be graded withour your initials below<br>\n",
    "I certify that this lab represents my own work and I have read the RU academic intergrity policies at<br>\n",
    "<a href=\"https://www.cs.rutgers.edu/academic-integrity/introduction\">https://www.cs.rutgers.edu/academic-integrity/introduction </a><br>\n",
    "<b>Initials</b>:     \n",
    "AR\n",
    "\n",
    "<h3>Grader Notes</h3>\n",
    "<b>Your Grade<b>:<br>\n",
    "<b>Grader Initials</b>:<br>\n",
    "<b>Grader Comments</b> (optional):<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Linear Regression, Gradient Descent and Feature Engineering\n",
    "\n",
    "### Due Date: Sunday Dec 12, 2021 on or before 11:59 PM\n",
    "\n",
    "In this lab we will work through the process of:\n",
    "1. implementing a linear model\n",
    "2. defining loss functions using L1, L2 and Huber\n",
    "3. Implementing a gradient descent \n",
    "4. Experiment with Feature Engineering\n",
    "5. Examining regression using numeric libraries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "In this part of the lab, we will model linear regression based on a data set that contains grades from CS 205 course in fall 2018. The dataset (with no ID's) contain midterm and final exam grades (and other assignments). \n",
    "# Task 1 - Initialization\n",
    "Read the file into a dataframe and keep only the midtermRaw and FinaRaw columns. We will be doing univariate regression on input=midterm, output=finalExam\n",
    "The goal is to find a model that will allow us to predict the final exam score given the midterm score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1.1  Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midtermRaw</th>\n",
       "      <th>finalRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   midtermRaw  finalRaw\n",
       "3        45.5      62.0\n",
       "4        58.0      60.5\n",
       "5        68.0      32.0\n",
       "6        64.5      50.5\n",
       "7        74.0      51.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CS205_grades_12_19_18_Final.csv\")\n",
    "df_cleaned = df[['midtermRaw','finalRaw']]\n",
    "# drop all undefined rows \n",
    "df_cleaned = df_cleaned.dropna() \n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1.2 Normalize Data\n",
    "Normalize midterm and final scores to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37857143]\n",
      " [0.55714286]\n",
      " [0.7       ]\n",
      " [0.65      ]\n",
      " [0.78571429]\n",
      " [0.47857143]\n",
      " [0.32142857]\n",
      " [0.80714286]\n",
      " [0.58571429]\n",
      " [0.06428571]\n",
      " [0.53571429]\n",
      " [0.46428571]\n",
      " [0.42142857]\n",
      " [0.83571429]\n",
      " [0.35      ]\n",
      " [0.        ]\n",
      " [0.65      ]\n",
      " [0.48571429]\n",
      " [0.55      ]\n",
      " [0.49285714]\n",
      " [1.        ]\n",
      " [0.6       ]\n",
      " [0.31428571]\n",
      " [0.61428571]\n",
      " [0.55714286]\n",
      " [0.48571429]\n",
      " [0.04285714]\n",
      " [0.5       ]\n",
      " [0.23571429]\n",
      " [0.73571429]\n",
      " [0.59285714]\n",
      " [0.48571429]\n",
      " [0.15714286]\n",
      " [0.83571429]\n",
      " [0.14285714]\n",
      " [0.35      ]\n",
      " [0.41428571]\n",
      " [0.62142857]\n",
      " [1.        ]\n",
      " [0.70714286]\n",
      " [0.56428571]\n",
      " [0.9       ]\n",
      " [0.73571429]\n",
      " [0.24285714]\n",
      " [0.41428571]\n",
      " [0.65      ]\n",
      " [0.52142857]\n",
      " [0.65714286]\n",
      " [0.38571429]\n",
      " [0.4       ]\n",
      " [0.55      ]\n",
      " [0.52857143]\n",
      " [0.46428571]\n",
      " [0.81428571]\n",
      " [0.6       ]\n",
      " [0.54285714]\n",
      " [0.1       ]\n",
      " [0.50714286]\n",
      " [0.2       ]\n",
      " [0.63571429]\n",
      " [0.25714286]\n",
      " [0.94285714]\n",
      " [0.36428571]\n",
      " [0.53571429]\n",
      " [0.8       ]\n",
      " [0.34285714]\n",
      " [0.65      ]\n",
      " [0.67142857]\n",
      " [0.60714286]\n",
      " [0.31428571]\n",
      " [0.72142857]\n",
      " [0.55714286]\n",
      " [0.78571429]\n",
      " [0.31428571]\n",
      " [0.95714286]\n",
      " [0.17857143]\n",
      " [0.68571429]\n",
      " [0.44285714]\n",
      " [0.69285714]\n",
      " [0.17857143]\n",
      " [0.47142857]\n",
      " [0.52857143]\n",
      " [0.27142857]\n",
      " [0.45      ]\n",
      " [0.52857143]\n",
      " [0.48571429]\n",
      " [0.74285714]\n",
      " [0.25714286]\n",
      " [0.85714286]\n",
      " [0.29285714]\n",
      " [0.58571429]\n",
      " [0.70714286]\n",
      " [0.65714286]\n",
      " [0.27857143]\n",
      " [0.78571429]\n",
      " [0.25      ]\n",
      " [0.3       ]\n",
      " [0.58571429]\n",
      " [0.97142857]\n",
      " [0.02857143]\n",
      " [0.95      ]\n",
      " [0.44285714]\n",
      " [0.34285714]\n",
      " [0.48571429]\n",
      " [0.40714286]\n",
      " [0.7       ]\n",
      " [0.19285714]\n",
      " [0.37857143]\n",
      " [0.82142857]\n",
      " [0.87142857]\n",
      " [0.47857143]\n",
      " [0.04285714]\n",
      " [0.93571429]\n",
      " [0.47857143]\n",
      " [0.74285714]\n",
      " [0.63571429]\n",
      " [0.35714286]\n",
      " [0.82857143]\n",
      " [0.51428571]\n",
      " [0.22142857]\n",
      " [0.79285714]\n",
      " [0.92142857]\n",
      " [0.6       ]] [[0.79104478]\n",
      " [0.76865672]\n",
      " [0.34328358]\n",
      " [0.61940299]\n",
      " [0.62686567]\n",
      " [0.39552239]\n",
      " [0.43283582]\n",
      " [0.82835821]\n",
      " [0.6641791 ]\n",
      " [0.26119403]\n",
      " [0.74626866]\n",
      " [0.60447761]\n",
      " [0.79850746]\n",
      " [0.73134328]\n",
      " [0.36567164]\n",
      " [0.42537313]\n",
      " [0.63432836]\n",
      " [0.70895522]\n",
      " [0.81343284]\n",
      " [0.55223881]\n",
      " [0.76119403]\n",
      " [0.47014925]\n",
      " [0.53731343]\n",
      " [0.60447761]\n",
      " [0.20895522]\n",
      " [0.44776119]\n",
      " [0.17910448]\n",
      " [0.31343284]\n",
      " [0.35074627]\n",
      " [0.98507463]\n",
      " [0.5       ]\n",
      " [0.43283582]\n",
      " [0.5       ]\n",
      " [0.86567164]\n",
      " [0.40298507]\n",
      " [0.61940299]\n",
      " [0.6641791 ]\n",
      " [0.6119403 ]\n",
      " [1.        ]\n",
      " [0.45522388]\n",
      " [0.56716418]\n",
      " [0.65671642]\n",
      " [0.88059701]\n",
      " [0.45522388]\n",
      " [0.60447761]\n",
      " [0.53731343]\n",
      " [0.79850746]\n",
      " [0.44776119]\n",
      " [0.55970149]\n",
      " [0.76119403]\n",
      " [0.75373134]\n",
      " [0.67164179]\n",
      " [0.42537313]\n",
      " [0.7761194 ]\n",
      " [0.71641791]\n",
      " [0.6119403 ]\n",
      " [0.43283582]\n",
      " [0.61940299]\n",
      " [0.32835821]\n",
      " [0.46268657]\n",
      " [0.50746269]\n",
      " [0.47014925]\n",
      " [0.45522388]\n",
      " [0.48507463]\n",
      " [0.60447761]\n",
      " [0.47014925]\n",
      " [0.82089552]\n",
      " [0.88059701]\n",
      " [0.6641791 ]\n",
      " [0.38059701]\n",
      " [0.75373134]\n",
      " [0.50746269]\n",
      " [0.52238806]\n",
      " [0.53731343]\n",
      " [0.81343284]\n",
      " [0.26119403]\n",
      " [0.69402985]\n",
      " [0.51492537]\n",
      " [0.55223881]\n",
      " [0.49253731]\n",
      " [0.34328358]\n",
      " [0.51492537]\n",
      " [0.54477612]\n",
      " [0.34328358]\n",
      " [0.64179104]\n",
      " [0.74626866]\n",
      " [0.73134328]\n",
      " [0.15671642]\n",
      " [0.95522388]\n",
      " [0.53731343]\n",
      " [0.68656716]\n",
      " [0.40298507]\n",
      " [0.39552239]\n",
      " [0.37313433]\n",
      " [0.93283582]\n",
      " [0.31343284]\n",
      " [0.41791045]\n",
      " [0.38059701]\n",
      " [0.78358209]\n",
      " [0.        ]\n",
      " [0.81343284]\n",
      " [0.2761194 ]\n",
      " [0.50746269]\n",
      " [0.1641791 ]\n",
      " [0.49253731]\n",
      " [0.76119403]\n",
      " [0.51492537]\n",
      " [0.64925373]\n",
      " [0.92537313]\n",
      " [0.58955224]\n",
      " [0.82089552]\n",
      " [0.36567164]\n",
      " [0.85074627]\n",
      " [0.67910448]\n",
      " [0.6641791 ]\n",
      " [0.71641791]\n",
      " [0.44029851]\n",
      " [0.89552239]\n",
      " [0.67910448]\n",
      " [0.29850746]\n",
      " [0.76119403]\n",
      " [0.54477612]\n",
      " [0.44776119]]\n"
     ]
    }
   ],
   "source": [
    "# set of observations\n",
    "X = df_cleaned.dropna().iloc[:,[0]]\n",
    "# set of categories\n",
    "Y = df_cleaned.dropna().iloc[:,[1]]\n",
    "\n",
    "# normalize the numeric data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "# BEGIN SOLUTION\n",
    "X_scaled_values = scaler.fit_transform(X)\n",
    "Y_scaled_values = scaler.fit_transform(Y)\n",
    "# END SOLUTION\n",
    "\n",
    "print(X_scaled_values, Y_scaled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1.3 Plot the data to see if a linear regression line is a good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9b3adc6730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "plt.scatter(X_scaled_values, Y_scaled_values)\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3.1\n",
    "**Based on what you see in the plot, do you think it is fine to use linear regression?. Briefly explain below**\n",
    "\n",
    "***BEGIN SOLUTION*** \n",
    "\n",
    "There appears to be a consistent upward trend and lots of observations that follow this trend. It would be interesting to use linear regression here to map this relationship and possibly train it on more datasets for further accuracy.\n",
    "\n",
    "***END SOLUTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Manual Exploration of Linear Regression Line\n",
    "In this task we will manually explore the linear regression line. This will give us a good intution about the process.\n",
    "The goal now is to fit a line \n",
    "$$\n",
    "h(\\theta) = \\theta_0 + \\theta_1*x \n",
    "$$\n",
    "for all data points (x,y), such that the error\n",
    "$$\n",
    "E(\\theta) = \\sum(h(\\theta)-y)^2 $$ is minimized. In this task we will manually change the values of theta0 and theta1 such that you obtained the smallest possible error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(theta0, theta1, x):\n",
    "    \"\"\"\n",
    "    Return the model theta0 + theta1*x\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "    return theta0+theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.1 - Define the square error (L2) function\n",
    "Define the function, sqerror, that computes the error based on the two arguments provided. The function h($\\theta$) is as defined above. Assume that x and y are the observed vectors. We use the square error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8891990884384056"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sqerror(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Returns: square error sum\n",
    "    Assumptions: x, y vectors global\n",
    "    \"\"\"\n",
    "    ## BEGIN SOLUTION\n",
    "    L = lambda a: a ** 2\n",
    "    y_pre = h(theta0, theta1, X_scaled_values)\n",
    "    err_lst = Y_scaled_values - y_pre\n",
    "    return float(sum(L(err_lst)))\n",
    "    ## END SOLUTION\n",
    "\n",
    "## testing\n",
    "sqerror(0.29,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.2 - Define the L1 error function\n",
    "Define the function, abserror, that computes the average absolute error based on the two arguments provided. The function h($\\theta$) is as defined above. Assume that x and y are the observed vectors. We use the average abssolute error in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.484200426439227"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def abserror(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Returns: square error sum\n",
    "    Assumptions: x, y vectors global\n",
    "    \"\"\"\n",
    "    ## BEGIN SOLUTION\n",
    "    L = lambda a: abs(a)\n",
    "    y_pre = h(theta0, theta1, X_scaled_values)\n",
    "    err_lst = Y_scaled_values - y_pre\n",
    "    return float(sum(L(err_lst)))\n",
    "    \n",
    "    ## END SOLUTION\n",
    "\n",
    "## testing\n",
    "abserror(0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.3 - Define the Pseudo Huber loss function\n",
    "Define the function, phuberror, that computes the pseudo huber error based on the two arguments provided. The function h($\\theta$) is as defined above. Assume that x and y are the observed vectors. We use the average huber error in this case. The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "where a is the absolute difference between predicted and observed (that is, h($\\theta$) - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8602900328694063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def phuberror(theta0, theta1, delta):\n",
    "    \"\"\"\n",
    "    Input: parameters theta0 and theta1 of the model \n",
    "    Returns: square error sum\n",
    "    Assumptions: x, y vectors global\n",
    "    \"\"\"\n",
    "    ## BEGIN SOLUTION\n",
    "    #L = lambda a: math.sqrt(a)\n",
    "    y_pre = h(theta0, theta1, X_scaled_values)\n",
    "    err_lst = Y_scaled_values - y_pre\n",
    "    inside = np.sqrt(1+ (abs(err_lst)/delta)**2)\n",
    "    return float(sum((delta**2) * (inside-1)))\n",
    "    ## END SOLUTION\n",
    "\n",
    "## testing\n",
    "phuberror(0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.4 Interactive Exploration.\n",
    "Let us initialize the interactive widget (used in Lab5) to create sliders that allows us to change the values of theta0 and theta1 and see how things change. Complete the function f below. The function is expected to get two values alpha(theta0) and beta(theta1) and plot both the points and the line on the same plot. It also needs to compute the error and display and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9677c9bdd85442ee814527393d45a99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=1.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interactive panel\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "\n",
    "def f(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Plot the line and points in an interactive panel\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    y_pre = h(theta0, theta1, X_scaled_values)\n",
    "    pylab.plot(X_scaled_values, y_pre)\n",
    "    pylab.scatter(X_scaled_values, Y_scaled_values, alpha=0.1)\n",
    "    sqerr = sqerror(theta0,theta1)\n",
    "    l1err = abserror(theta0,theta1)\n",
    "    huber = phuberror(theta0,theta1,0.1) #maintaining delta of 0.1\n",
    "    print(\"L2/Square Error: {} \".format(sqerr) )\n",
    "    print(\"L1/Abs Error: {} \".format(l1err) )\n",
    "    print(\"Huber Error: {} \".format(huber) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # END SOLUTION\n",
    "\n",
    "interact(f, theta1=(0,3,0.1), theta0=(-0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.5 Record the best values for each error function\n",
    "Write the \"best\" values you found for theta0 (y-intercept) and theta1 (slope) and the error. \n",
    "This error is the minimum you have observed based on the manual exploration using the widget \n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE THE DOTTED LINES WITH YOUR ANSWERS\n",
    "\n",
    "# BEST VALUES FOR SQUARE ERROR\n",
    "theta0 = 0.3\n",
    "theta1 = 0.5\n",
    "error = 2.89\n",
    "\n",
    "# BEST VALUES FOR ABS ERROR\n",
    "theta0 = 0.3\n",
    "theta1 = 0.5\n",
    "error = 15.5\n",
    "\n",
    "# BEST VALUES FOR HUBER ERROR\n",
    "theta0 = 0.3\n",
    "theta1 = 0.5\n",
    "error = 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Gradient Descent\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the graph. The iterative formulas are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0)-y_j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0 - y_j)*x_j\n",
    "$$\n",
    "\n",
    "The alpha ($\\alpha$) is called the \"learning rate\". It is important to pick a good value for alpha so that convergence is not too slow (small alpha) or at the risk of over shooting the minimum point (large alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3.1 Compute Gradient Descent (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.302348766725856, 0.5127565218727459)\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and error threshold (how many decimals of accuracy) \n",
    "# the function returns theta0 and theta1\n",
    "# that minimizes the error.\n",
    "\n",
    "def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Return theta0, and theta1 from Gradient Descent algorithm\n",
    "    \"\"\"\n",
    "    theta0, theta1 = 0, 0\n",
    "    # BEGIN SOLUTION\n",
    "    sum_theta0 = lambda a,b : float(sum(b*x+a-y for x,y in zip(obsX, obsY)))\n",
    "    sum_theta1 = lambda a,b : float(sum((b*x+a-y)*x for x,y in zip(obsX, obsY)))\n",
    "\n",
    "    prev_theta0, prev_theta1 = 0, 0\n",
    "    oldError = sqerror(prev_theta0, prev_theta1)\n",
    "    newError = oldError + 100\n",
    "    iterations = 0\n",
    "\n",
    "#print(f'iter_num = {iterations}, theta0 = {prev_theta0}, theta1 = {prev_theta1}')\n",
    "\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        theta0 = prev_theta0 - alpha*sum_theta0(prev_theta0, prev_theta1)\n",
    "        theta1 = prev_theta1 - alpha*sum_theta1(prev_theta0, prev_theta1)\n",
    "    \n",
    "        newError = sqerror(theta0, theta1)\n",
    "        oldError = sqerror(prev_theta0, prev_theta1)\n",
    "\n",
    "        prev_theta0 = theta0\n",
    "        prev_theta1 = theta1\n",
    "    \n",
    "        iterations += 1\n",
    "        #print(f'iter_num = {iterations}, theta0 = {theta0}, theta1 = {theta1}, newError = {newError}')\n",
    "\n",
    "\n",
    "    \n",
    "    # END SOLUTION        \n",
    "    return theta0,theta1\n",
    "    \n",
    "#testing\n",
    "print(gd2(X_scaled_values,Y_scaled_values,0.01,0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3.2 Compute Gradient Descent (Huber)\n",
    "First, compute a formula for the huber gradient descent using similar derivative methods used in gradient descent for L2 loss (we did derivatives for L2 loss in the lecture. You need to do the derivatives for humber function as defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3074733503633054, 0.5043527349174003)\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), error change threshold,\n",
    "# and delta from the huber loss model,\n",
    "# the function returns theta0 and theta1\n",
    "# that minimizes the error.\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Return theta0 and theta1, error from huber gradient Descent algorithm\n",
    "    \"\"\"\n",
    "    theta0, theta1 = 0, 0\n",
    "    # BEGIN SOLUTION\n",
    "    sum_theta0 = lambda a,b : float(sum(b*x+a-y for x,y in zip(obsX, obsY)))\n",
    "    sum_theta1 = lambda a,b : float(sum((b*x+a-y)*x for x,y in zip(obsX, obsY)))\n",
    "\n",
    "    prev_theta0, prev_theta1 = 0, 0\n",
    "    oldError = phuberror(prev_theta0, prev_theta1,delta)\n",
    "    newError = oldError + 100\n",
    "    iterations = 0\n",
    "\n",
    "#print(f'iter_num = {iterations}, theta0 = {prev_theta0}, theta1 = {prev_theta1}')\n",
    "\n",
    "    while abs(newError - oldError) >= threshold:\n",
    "        theta0 = prev_theta0 - alpha*sum_theta0(prev_theta0, prev_theta1)\n",
    "        theta1 = prev_theta1 - alpha*sum_theta1(prev_theta0, prev_theta1)\n",
    "    \n",
    "        newError = phuberror(theta0, theta1, delta)\n",
    "        oldError = phuberror(prev_theta0, prev_theta1, delta)\n",
    "\n",
    "        prev_theta0 = theta0\n",
    "        prev_theta1 = theta1\n",
    "    \n",
    "        iterations += 1\n",
    "        #print(f'iter_num = {iterations}, theta0 = {theta0}, theta1 = {theta1}, newError = {newError}')\n",
    "\n",
    "\n",
    "\n",
    "    # END SOLUTION \n",
    "    return theta0,theta1\n",
    "\n",
    "\n",
    "# testing\n",
    "print(gdh(X_scaled_values,Y_scaled_values,0.001,0.0001,1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.1. Write the values of theta0, theta1, alpha, error that provided the minimum value through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE THE DOTTED LINES WITH YOUR ANSWERS\n",
    "\n",
    "# L2 ERROR\n",
    "theta0 = ...\n",
    "theta1 = ...\n",
    "alpha = ...\n",
    "error = ...\n",
    "\n",
    "# HUBER ERROR\n",
    "theta0 = ...\n",
    "theta1 = ...\n",
    "alpha = ...\n",
    "error = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Experiment with the new values of theta0, theta1 to see if the interactive widget shows similar things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c70fd926810445997771deb3c69f06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=1.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(f, theta1=(0,3,0.1), theta0=(-0,1,0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3.4 Compare with Library Estimators (just for L2 error)\n",
    "Now use the sklearn LinearRegression module to automate this process. What coefficients do you get? Are they close what you received from gradient descent? Find the error from sklearn package. Is that error smaller or bigger than the squared error you received? Write your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29620134]\n",
      "[[0.5238794]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(X_scaled_values,Y_scaled_values)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8807167362296533"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to the Activity 3.4 questions here\n",
    "\n",
    "***BEGIN SOLUTION***\n",
    "\n",
    "\n",
    "***END SOLUTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3.5 - Predict Your Final Exam Score\n",
    "The regression line was obtained using grades from CS 205 course. We can consider them to be training data. Now we trained a model (with theta0 and theta1) so we can predict the grade for your own course (CS 439) based on your midterm grade.\n",
    "We will do few things before we can accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Read the midterm grades\n",
    "The grade file for CS439 midterm is given in data/CS439_grades.csv. Read this data file to a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   midterm  97 non-null     float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 904.0 bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_439 = pd.read_csv(\"data/CS439_grades_03_15_19.csv\")\n",
    "df_439.info()\n",
    "mid = df_439[df_439['midterm']<80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Predict your Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midterm Score :  61.5 \t Predicted Score :  61.858866076959785\n",
      "Midterm Score :  42.3 \t Predicted Score :  51.78426215207834\n",
      "Midterm Score :  52.6 \t Predicted Score :  57.15738424534843\n",
      "Midterm Score :  12.8 \t Predicted Score :  36.33653613392679\n",
      "Midterm Score :  94.9 \t Predicted Score :  79.3215128800876\n",
      "Midterm Score :  56.4 \t Predicted Score :  59.17230503032472\n",
      "Midterm Score :  39.7 \t Predicted Score :  50.44098162876081\n",
      "Midterm Score :  46.2 \t Predicted Score :  53.79918293705461\n",
      "Midterm Score :  43.6 \t Predicted Score :  52.4559024137371\n",
      "Midterm Score :  21.8 \t Predicted Score :  41.03801796553813\n",
      "Midterm Score :  48.7 \t Predicted Score :  55.14246346037215\n",
      "Midterm Score :  43.6 \t Predicted Score :  52.4559024137371\n",
      "Midterm Score :  75.6 \t Predicted Score :  69.24690895520615\n",
      "Midterm Score :  65.4 \t Predicted Score :  63.873786861936054\n",
      "Midterm Score :  52.6 \t Predicted Score :  57.15738424534843\n",
      "Midterm Score :  43.6 \t Predicted Score :  52.4559024137371\n",
      "Midterm Score :  61.5 \t Predicted Score :  61.858866076959785\n",
      "Midterm Score :  42.3 \t Predicted Score :  51.78426215207834\n",
      "Midterm Score :  60.3 \t Predicted Score :  61.18722581530102\n",
      "Midterm Score :  100.0 \t Predicted Score :  82.00807392672264\n",
      "Midterm Score :  70.5 \t Predicted Score :  66.5603479085711\n",
      "Midterm Score :  61.5 \t Predicted Score :  61.858866076959785\n",
      "Midterm Score :  0.0 \t Predicted Score :  29.62013351733917\n",
      "Midterm Score :  53.8 \t Predicted Score :  57.8290245070072\n",
      "Midterm Score :  34.6 \t Predicted Score :  47.75442058212577\n",
      "Midterm Score :  83.3 \t Predicted Score :  73.27675052515875\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  91.0 \t Predicted Score :  77.3065920951113\n",
      "Midterm Score :  29.5 \t Predicted Score :  45.06785953549071\n",
      "Midterm Score :  73.1 \t Predicted Score :  67.90362843188863\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  2.6 \t Predicted Score :  30.963414040656694\n",
      "Midterm Score :  61.5 \t Predicted Score :  61.858866076959785\n",
      "Midterm Score :  69.2 \t Predicted Score :  65.88870764691235\n",
      "Midterm Score :  48.7 \t Predicted Score :  55.14246346037215\n",
      "Midterm Score :  98.7 \t Predicted Score :  81.3364336650639\n",
      "Midterm Score :  34.6 \t Predicted Score :  47.75442058212577\n",
      "Midterm Score :  60.3 \t Predicted Score :  61.18722581530102\n",
      "Midterm Score :  44.9 \t Predicted Score :  53.12754267539586\n",
      "Midterm Score :  50.0 \t Predicted Score :  55.81410372203091\n",
      "Midterm Score :  69.2 \t Predicted Score :  65.88870764691235\n",
      "Midterm Score :  60.3 \t Predicted Score :  61.18722581530102\n",
      "Midterm Score :  83.3 \t Predicted Score :  73.27675052515875\n",
      "Midterm Score :  26.9 \t Predicted Score :  43.72457901217318\n",
      "Midterm Score :  61.5 \t Predicted Score :  61.858866076959785\n",
      "Midterm Score :  20.5 \t Predicted Score :  40.36637770387937\n",
      "Midterm Score :  19.2 \t Predicted Score :  39.69473744222061\n",
      "Midterm Score :  6.4 \t Predicted Score :  32.978334825632984\n",
      "Midterm Score :  80.8 \t Predicted Score :  71.93347000184121\n",
      "Midterm Score :  44.9 \t Predicted Score :  53.12754267539586\n",
      "Midterm Score :  26.9 \t Predicted Score :  43.72457901217318\n",
      "Midterm Score :  14.1 \t Predicted Score :  37.00817639558556\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  51.3 \t Predicted Score :  56.48574398368967\n",
      "Midterm Score :  24.4 \t Predicted Score :  42.381298488855656\n",
      "Midterm Score :  78.2 \t Predicted Score :  70.59018947852368\n",
      "Midterm Score :  23.1 \t Predicted Score :  41.7096582271969\n",
      "Midterm Score :  60.3 \t Predicted Score :  61.18722581530102\n",
      "Midterm Score :  71.8 \t Predicted Score :  67.23198817022987\n",
      "Midterm Score :  85.9 \t Predicted Score :  74.62003104847626\n",
      "Midterm Score :  50.0 \t Predicted Score :  55.81410372203091\n",
      "Midterm Score :  37.2 \t Predicted Score :  49.097701105443285\n",
      "Midterm Score :  73.1 \t Predicted Score :  67.90362843188863\n",
      "Midterm Score :  62.8 \t Predicted Score :  62.530506338618544\n",
      "Midterm Score :  59.0 \t Predicted Score :  60.51558555364225\n",
      "Midterm Score :  70.5 \t Predicted Score :  66.5603479085711\n",
      "Midterm Score :  5.1 \t Predicted Score :  32.306694563974226\n",
      "Midterm Score :  34.6 \t Predicted Score :  47.75442058212577\n",
      "Midterm Score :  19.2 \t Predicted Score :  39.69473744222061\n",
      "Midterm Score :  50.0 \t Predicted Score :  55.81410372203091\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  7.7 \t Predicted Score :  33.64997508729174\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  67.9 \t Predicted Score :  65.21706738525359\n",
      "Midterm Score :  53.8 \t Predicted Score :  57.8290245070072\n",
      "Midterm Score :  39.7 \t Predicted Score :  50.44098162876081\n",
      "Midterm Score :  43.6 \t Predicted Score :  52.4559024137371\n",
      "Midterm Score :  98.7 \t Predicted Score :  81.3364336650639\n",
      "Midterm Score :  79.5 \t Predicted Score :  71.26182974018245\n",
      "Midterm Score :  88.5 \t Predicted Score :  75.9633115717938\n",
      "Midterm Score :  55.1 \t Predicted Score :  58.500664768665956\n",
      "Midterm Score :  65.4 \t Predicted Score :  63.873786861936054\n",
      "Midterm Score :  57.7 \t Predicted Score :  59.84394529198349\n",
      "Midterm Score :  46.2 \t Predicted Score :  53.79918293705461\n",
      "Midterm Score :  51.9 \t Predicted Score :  56.82156411451905\n",
      "Midterm Score :  70.5 \t Predicted Score :  66.5603479085711\n",
      "Midterm Score :  51.3 \t Predicted Score :  56.48574398368967\n",
      "Midterm Score :  43.6 \t Predicted Score :  52.4559024137371\n",
      "Midterm Score :  6.4 \t Predicted Score :  32.978334825632984\n",
      "Midterm Score :  30.8 \t Predicted Score :  45.73949979714948\n",
      "Midterm Score :  96.2 \t Predicted Score :  79.99315314174635\n",
      "Midterm Score :  69.2 \t Predicted Score :  65.88870764691235\n",
      "Midterm Score :  64.1 \t Predicted Score :  63.202146600277295\n",
      "Midterm Score :  38.5 \t Predicted Score :  49.76934136710205\n",
      "Midterm Score :  42.3 \t Predicted Score :  51.78426215207834\n",
      "Midterm Score :  83.3 \t Predicted Score :  73.27675052515875\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X = mid.iloc[:,[0]]\n",
    "scaler = MinMaxScaler()\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "# END SOLUTION\n",
    "\n",
    "n = len(X_scaled_values)\n",
    "newX = X_scaled_values.reshape(n)\n",
    "    \n",
    "for i in range(len(X_scaled_values)):\n",
    "    print('Midterm Score : ', round(100*newX[i],1), '\\t', 'Predicted Score : ',100*h(theta0, theta1, newX[i])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Feature Engineering\n",
    "In this task we will learn about feature engineering and how to use one-hot-encoding to analyze a data set.\n",
    "To begin with, we load the tips dataset from the seaborn library. The tips data contains records of tips, total bill, and personal information about the person who paid the bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records: 244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.1: Defining the Model and Feature Engineering\n",
    "Previously, we defined a simple linear model with only two parameters. Now let's make a more complicated model that utilizes other features in a dataset. Let our prediction for tip be a combination of the following features:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 * \\text{total_bill} + \\theta_2 * \\text{sex} + \\theta_3 * \\text{smoker} + \\theta_4 * \\text{day} + \\theta_5 * \\text{time} + \\theta_6 * \\text{size}\n",
    "$$\n",
    "\n",
    "Notice that some of these features are not numbers! But our linear model will need to predict a numerical value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Split the data into tips (column tip) and X (all other columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEGIN SOLUTION\n",
    "tips = ...\n",
    "X = ...\n",
    "## END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.2: Feature Engineering\n",
    "First, let's convert everything to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. For example, we can treat the day as a value from 1-7. However, one of the issues in directly translating to a numeric value is that we unintentially assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7. Monday is assigned to 1 and thus Sunday has 7 times the influence of Monday in our linear model which can lower the accuracy of our model. We will use **one-hot encoding** to address this issue (see lecture notes)\n",
    "\n",
    "One-hot encoding will produce a binary vector indicating the non-numeric feature. Sunday would be encoded as a [0 0 0 0 0 0 1]. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input, data. \n",
    "    Hint: check pd.get_dummies\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>total_bill</th>\n",
       "      <th>day_Thur</th>\n",
       "      <th>day_Fri</th>\n",
       "      <th>day_Sat</th>\n",
       "      <th>day_Sun</th>\n",
       "      <th>sex_Male</th>\n",
       "      <th>sex_Female</th>\n",
       "      <th>smoker_Yes</th>\n",
       "      <th>smoker_No</th>\n",
       "      <th>time_Lunch</th>\n",
       "      <th>time_Dinner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>16.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>21.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>23.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  total_bill  day_Thur  day_Fri  day_Sat  day_Sun  sex_Male  \\\n",
       "0     2       16.99         0        0        0        1         0   \n",
       "1     3       10.34         0        0        0        1         1   \n",
       "2     3       21.01         0        0        0        1         1   \n",
       "3     2       23.68         0        0        0        1         1   \n",
       "4     4       24.59         0        0        0        1         0   \n",
       "\n",
       "   sex_Female  smoker_Yes  smoker_No  time_Lunch  time_Dinner  \n",
       "0           1           0          1           0            1  \n",
       "1           0           0          1           0            1  \n",
       "2           0           0          1           0            1  \n",
       "3           0           0          1           0            1  \n",
       "4           1           0          1           0            1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.3: Defining the Model\n",
    "Now that all of our data is numeric, let's define our model function. Note that X and thetas are matrices now. Use matrix products to compute the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined above.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_model(np.arange(1,5), np.arange(1,5)) == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.4: Fitting the Model: Numerical Methods\n",
    "Recall in the lectures and in labs, we defined multiple loss functions and found optimal theta using the scipy.minimize function. Adapt the loss functions and optimization code from the previous lab (provided below) to work with your new linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1759912 , 0.09448701, 0.0151997 , 0.17746083, 0.05600931,\n",
       "       0.15199329, 0.1841108 , 0.2165523 , 0.15712756, 0.24353555,\n",
       "       0.23440131, 0.16626171])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def abserror(y, y_hat):\n",
    "    return np.abs(y-y_hat)\n",
    "\n",
    "def sqerror(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "def minimize_average_loss(loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    loss_function: either the squared or absolute loss functions from above.\n",
    "    model: the model (as defined above)\n",
    "    x: the x values (one-hot encoded data)\n",
    "    y: the y values (tip amounts)\n",
    "    return the estimate for each theta as a vector\n",
    "    \n",
    "    Note we will ignore failed convergence for this lab ... \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Notes on the following function call which you need to finish:\n",
    "    # \n",
    "    # 0. the ... should be replaced with the average loss evaluated on \n",
    "    #       the data x, y using the model and appropriate loss function\n",
    "    # 1. x0 are the initial values for THETA.  Yes, this is confusing\n",
    "    #       but optimization people like x to be the thing they are \n",
    "    #       optimizing.\n",
    "    # 2. We extract the 'x' entry in the dictionary which corresponds\n",
    "    #       to the value of thetas at the optimum\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n",
    "minimize_average_loss(sqerror, linear_model, one_hot_X, tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.5: Fitting the Model: Analytical Methods\n",
    "Let's also fit our model analytically, for the L2 loss function. In this question we will derive an analytical solution, fit our model and compare our results with our numerical optimization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1: Least Squares Solution\n",
    "Recall that if we're fitting a linear model with the l2 loss function, we are performing least squares! Remember, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} ||X\\theta - y||^2$$\n",
    "\n",
    "Let's begin by deriving the analytic solution to least squares. Write your answer in LaTeX in the cell below. Assume X is full column rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***BEGIN SOLUTION***\n",
    "\n",
    "$$ $$\n",
    "$$ $$\n",
    "$$ $$\n",
    "$$ $$\n",
    "***END SOLUTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2: Solving for Theta\n",
    "Now that we have the analytic solution for $\\theta$, let's find the optimal numerical thetas for our tips dataset. Fill out the function below. Make sure you use the float type in your calculations using .astype(float) and use the np.linalg.inv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical(x, y):\n",
    "    \"\"\"\n",
    "    x: our one-hot encoded dataset\n",
    "    y: tip amounts\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our analytical loss is:  454.4537426229509\n",
      "Our numerical loss is:  1.010353561236703\n"
     ]
    }
   ],
   "source": [
    "analytical_thetas = get_analytical(one_hot_X.astype(float), tips.astype(float))\n",
    "print(\"Our analytical loss is: \", sqerror(linear_model(analytical_thetas, one_hot_X),tips).mean())\n",
    "print(\"Our numerical loss is: \", sqerror(linear_model(minimize_average_loss(sqerror, linear_model, one_hot_X, tips), one_hot_X), tips).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4.5.3 Compare Analytical and Numerical Loss\n",
    "Our analytical loss is surprisingly much worse than our numerical loss. Why is this? Explain below.\n",
    "\n",
    "Hint: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "Please provide feedback on this lab.\n",
    "* how would you rate this lab (from 1-10, 10-highest) :\n",
    "* how can we improve his lab? :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as your_section_your_netID_lab6.jpynb<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; lab6 <br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Developed by A.D. Gunawardena @ 2019-2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
